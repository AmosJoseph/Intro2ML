{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WS2332 - Project 7 - Lecture 4\n",
    "Miguel Bessa\n",
    "<div>\n",
    "<img src=docs/tudelft_logo.jpg width=300px></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**What:** Lab Session 3 of course WS2332 (Project 7): Introduction to Machine Learning\n",
    "\n",
    "* Today's lecture focuses on **multidimensional** regression and classification via supervised learning\n",
    "\n",
    "**How:** Jointly workout this notebook\n",
    "* GitHub: https://github.com/mabessa/Intro2ML\n",
    "    1. You can do this locally in your computer (but you have to have the Python packages installed):\n",
    "        * clone the repository to your computer: git clone https://github.com/mabessa/Intro2ML\n",
    "        * load jupyter notebook (it will open in your internet browser): jupyter notebook\n",
    "        * search for this notebook in your computer and open it\n",
    "    2. Or you can use Google's Colab (no installation required, but times out if idle):\n",
    "        * go to https://colab.research.google.com\n",
    "        * login\n",
    "        * File > Open notebook\n",
    "        * click on Github (no need to login or authorize anything)\n",
    "        * paste the git link: https://github.com/mabessa/Intro2ML\n",
    "        * click search and then click on the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Outline for today\n",
    "\n",
    "1. Multidimensional regression with supervised learning\n",
    "2. Classification with supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 1. Multidimensional regression\n",
    "\n",
    "Similarly to what we did for one-dimension, now we are going to show how to do multidimensional regression with supervised learning.\n",
    "\n",
    "As you will see, it's pretty much the same thing...\n",
    "\n",
    "* We will use less obvious functions to generate our datasets for subsequent learning.\n",
    "\n",
    "\n",
    "* There is a nice website listing many different benchmark functions used in optimization: https://www.sfu.ca/~ssurjano/optimization.html\n",
    "\n",
    "Let's try to learn some of them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "# Aobe line is to enable us to rotate the surface plots.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm # to change colors of surface plots\n",
    "#\n",
    "# Let's define some functions that are often used to benchmark\n",
    "# algorithms (in optimization)\n",
    "def levy( x ):\n",
    "    function_name = 'Levy' # to output name of function\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    if x.ndim == 1:\n",
    "        x = np.reshape(x,(-1,2)) # reshape into 2d array\n",
    "    #\n",
    "    n_points, n_features = np.shape(x)\n",
    "    y = np.empty((n_points,1))\n",
    "    #\n",
    "    for ii in range(n_points):\n",
    "        z = 1 + (x[ii,:] - 1) / 4\n",
    "        y[ii] = (np.sin( np.pi * z[0] )**2\n",
    "            + sum( (z[:-1] - 1)**2 * (1 + 10 * np.sin( np.pi * z[:-1] + 1 )**2 ))\n",
    "            +       (z[-1] - 1)**2 * (1 + np.sin( 2 * np.pi * z[-1] )**2 ))\n",
    "    return (np.atleast_1d(y), function_name)\n",
    "#\n",
    "def ackley( x, a=20, b=0.2, c=2*np.pi ):\n",
    "    function_name = 'Ackley' # to output name of function\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    if x.ndim == 1:\n",
    "        x = np.reshape(x,(-1,2)) # reshape into 2d array\n",
    "    #\n",
    "    n_points, n_features = np.shape(x)\n",
    "    y = np.empty((n_points,1))\n",
    "    #\n",
    "    for ii in range(n_points):\n",
    "        s1 = sum( x[ii,:]**2 )\n",
    "        s2 = sum( np.cos( c * x[ii,:] ))\n",
    "        y[ii] = -a*np.exp( -b*np.sqrt( s1 / n_features )) - np.exp( s2 / n_features ) + a + np.exp(1)\n",
    "    return (np.atleast_1d(y), function_name)\n",
    "#\n",
    "def rosenbrock( x ):  # rosen.m\n",
    "    function_name = 'Rosenbrock' # to output name of function\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    if x.ndim == 1:\n",
    "        x = np.reshape(x,(-1,2)) # reshape into 2d array\n",
    "    #\n",
    "    n_points, n_features = np.shape(x)\n",
    "    y = np.empty((n_points,1))\n",
    "    #\n",
    "    for ii in range(n_points):\n",
    "        x0 = x[ii,:-1]\n",
    "        x1 = x[ii,1:]\n",
    "        y[ii] = (sum( (1 - x0) **2 )\n",
    "            + 100 * sum( (x1 - x0**2) **2 ))\n",
    "    return (np.atleast_1d(y), function_name)\n",
    "#\n",
    "def schwefel( x ):  # schw.m\n",
    "    function_name = 'Schwefel' # to output name of function\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    if x.ndim == 1:\n",
    "        x = np.reshape(x,(-1,2)) # reshape into 2d array\n",
    "    #\n",
    "    n_points, n_features = np.shape(x)\n",
    "    y = np.empty((n_points,1))\n",
    "    #\n",
    "    for ii in range(n_points):\n",
    "        y[ii] = 418.9829*n_features - sum( x[ii,:] * np.sin( np.sqrt( abs( x[ii,:] ))))\n",
    "    return (np.atleast_1d(y), function_name)\n",
    "#\n",
    "def griewank( x, fr=4000 ):\n",
    "    function_name = 'Griewank' # to output name of function\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    if x.ndim == 1:\n",
    "        x = np.reshape(x,(-1,2)) # reshape into 2d array\n",
    "    #\n",
    "    n_points, n_features = np.shape(x)\n",
    "    y = np.empty((n_points,1))\n",
    "    #\n",
    "    j = np.arange( 1., n_features+1 )\n",
    "    for ii in range(n_points):\n",
    "        s = sum( x[ii,:]**2 )\n",
    "        p = np.prod( np.cos( x[ii,:] / np.sqrt(j) ))\n",
    "        y[ii] = s/fr - p + 1\n",
    "    return (np.atleast_1d(y), function_name)\n",
    "#\n",
    "def rastrigin( x ):  # rast.m\n",
    "    function_name = 'Rastrigin' # to output name of function\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    if x.ndim == 1:\n",
    "        x = np.reshape(x,(-1,2)) # reshape into 2d array\n",
    "    #\n",
    "    n_points, n_features = np.shape(x)\n",
    "    y = np.empty((n_points,1))\n",
    "    #\n",
    "    for ii in range(n_points):\n",
    "        y[ii] = 10*n_features + sum( x[ii,:]**2 - 10 *\n",
    "                                np.cos( 2 * np.pi * x[ii,:] ))\n",
    "    return (np.atleast_1d(y), function_name)\n",
    "#\n",
    "# end of functions for benchmark.\n",
    "\n",
    "#\n",
    "seed = 1987 # set a random seed to replicate results\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by considering equally spaced sampling points, as we did in our previous lectures (for 1D regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform grid of points for two-dimensional input\n",
    "n_points_per_dimension = 50\n",
    "x1 = np.linspace(-10.0, 10.0, n_points_per_dimension)\n",
    "x2 = np.linspace(-10.0, 10.0, n_points_per_dimension)\n",
    "\n",
    "xx1, xx2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# Input points reshaped for Pandas dataframe\n",
    "input_points = np.array([xx1, xx2]).reshape(2, -1).T\n",
    "\n",
    "# Output data created from one of the benchmark functions:\n",
    "# levy, ackley, rosenbrock, schwefel, griewank, rastrigin\n",
    "output_points, function_name = schwefel(input_points)\n",
    "\n",
    "print(np.shape(output_points))\n",
    "print(np.shape(xx1))\n",
    "\n",
    "# Reshape output data for use in Surface plot:\n",
    "yy1 = np.reshape(output_points,np.shape(xx1))\n",
    "\n",
    "# Set the color scheme used in every plot:\n",
    "set_cm = cm.cool # viridis, inferno, copper, PuBu, cool, coolwarm, hsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the function in a few different ways.\n",
    "\n",
    "1. Figure 1 with two subplots one on top of the other. The top subplot is the 3D surface of the function, while the bottom subplot is just a wireframe of the surface (no colors)\n",
    "\n",
    "2. Figure 2 is a contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1\n",
    "fig1 = plt.figure(figsize=plt.figaspect(2.0))\n",
    "\n",
    "# Subplot 1 (top) of Figure 1\n",
    "ax1 = fig1.add_subplot(2, 1, 1, projection='3d')\n",
    "# Surface plot:\n",
    "surf = ax1.plot_surface(xx1, xx2, yy1,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "# Create axis labels and title:\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(x_1,x_2)$')\n",
    "ax1.set_title(\"%s function\" % function_name)\n",
    "\n",
    "# Subplot 2 (bottom) of Figure 1\n",
    "ax2 = fig1.add_subplot(2, 1, 2, projection='3d')\n",
    "# Plot a 3D wireframe (no colors)\n",
    "ax2.plot_wireframe(xx1, xx2, np.reshape(output_points,np.shape(xx1)),\n",
    "                   rstride=5, cstride=5)\n",
    "# Create axis labels and title:\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_zlabel('$f(x_1,x_2)$')\n",
    "ax2.set_title(\"%s function\" % function_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# Figure 2\n",
    "# Create Contour plot:\n",
    "fig2 = plt.figure()\n",
    "ax3 = fig2.add_subplot(1, 1, 1)\n",
    "cset = ax3.contourf(xx1, xx2, yy1, cmap=set_cm)\n",
    "ax3.set_xlabel('$x_1$')\n",
    "ax3.set_ylabel('$x_2$')\n",
    "ax3.set_title(\"%s function\" % function_name)\n",
    "fig2.colorbar(cset, ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get on with the regression tasks, let's introduce a very common way to handle datasets in data science: the Pandas DataFrame.\n",
    "\n",
    "* Let's create a pandas dataframe for the above dataset, i.e. for the inputs (features 'x1' and 'x2') and outputs (target 'y1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Pandas dataframe (very common in ML)\n",
    "#\n",
    "# Create a dictionary with the names of the input and output variables\n",
    "# and their respective values:\n",
    "input_dictionary = {\n",
    "            'x1' : input_points[:,0],\n",
    "            'x2' : input_points[:,1],\n",
    "            'y1' : output_points[:,0]\n",
    "            }\n",
    "#\n",
    "# Then, we create a Pandas data frame:\n",
    "df = pd.DataFrame(input_dictionary)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrame is a quite powerful way to handle data. For example:\n",
    "\n",
    "* It deals with different kinds of data (not just numbers)\n",
    "\n",
    "\n",
    "* It includes labels for each feature (input) and target (output). You can think about it like an organized way to create an 'Excel' table that we can easily access.\n",
    "\n",
    "There is a lot more to it, of course...\n",
    "\n",
    "Here, we will just highlight a few ways to handle a Pandas DataFrame:\n",
    "\n",
    "1. Direct way to select columns & rows by how they were labeled originaly\n",
    "\n",
    "\n",
    "2. DataFrame.loc to select columns & rows by Name\n",
    "\n",
    "\n",
    "3. DataFrame.iloc to select columns & rows by Index Positions (integer numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's access every row of feature 'x1' and of target 'y1':\n",
    "\n",
    "way1 = df[['x1','y1']] # direct way\n",
    "\n",
    "way2 = df.loc[:,['x1','y1']] # using the labels of rows and columns\n",
    "\n",
    "way3 = df.iloc[:,[0,2]] # using indices (integers) of rows and columns\n",
    "\n",
    "print('way 1 =\\n', way1, '\\n\\n') # the '\\n' is to\n",
    "                                 # make a new line for\n",
    "                                 # visualization purposes only.\n",
    "print('way 2 =\\n', way2, '\\n\\n')\n",
    "print('way 3 =\\n', way3, '\\n\\n')\n",
    "\n",
    "# Now let's access rows 2 until 5 (inclusive) of feature 'x2' and\n",
    "# target 'y1':\n",
    "\n",
    "# I think we can't do that by the direct way (correct me if I am wrong)\n",
    "\n",
    "way2 = df.loc[2:5,['x2','y1']] # using the labels of data. Note that\n",
    "                               # here when we write 2:5 (it includes)\n",
    "                               # row with index 5\n",
    "\n",
    "way3 = df.iloc[2:6,[1,2]] # using indices (integers) of the data\n",
    "                          # note that we write 2:6 to include 5,\n",
    "                          # which differs from way2 (!)\n",
    "                          # Accessing 'x2' and 'y1' could also be\n",
    "                          # done by writing 1:3 instead of [1,2]\n",
    "# way3 = df.iloc[2:6,1:3] # alternative way to access 'x2' and 'y1'\n",
    "\n",
    "print('Specific rows, way 2 =\\n', way2, '\\n\\n')\n",
    "print('Specific rows, way 3 =\\n', way3, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our data into numpy variables from our Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df.loc[:,['x1','x2']].values # note that we ask for the\n",
    "                                      # values, not a subset of the\n",
    "                                      # DataFrame\n",
    "y_data = df.loc[:,'y1'].values\n",
    "\n",
    "print(X_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "<a id='#split_dataset'></a>\n",
    "\n",
    "As usual, split the dataset into training and testing sets.\n",
    "\n",
    "* For now, let's split the data with the following ratio: 2% for training set, and 98% for testing set\n",
    "\n",
    "Note: the commonly used ratio is 75% for training and 25% for testing, but we will use Gaussian Processes and they from very little data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "# Write here.\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous class, it is good practice to scale the dataset (e.g. in Neural Networks this is very important).\n",
    "\n",
    "So, let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling inputs with a Standard Scaler:\n",
    "scaler_x = StandardScaler().fit(X_train)\n",
    "#\n",
    "X_train_scaled=scaler_x.transform(X_train)\n",
    "X_test_scaled=scaler_x.transform(X_test)\n",
    "X_data_scaled=scaler_x.transform(X_data)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Scaling outputs with a MinMax Scaler (just to show a different one!):\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "y_data = y_data.reshape(-1, 1)\n",
    "#\n",
    "scaler_y = MinMaxScaler(feature_range=(-1,1)).fit(y_train)\n",
    "#\n",
    "y_train_scaled=scaler_y.transform(y_train)\n",
    "y_test_scaled=scaler_y.transform(y_test)\n",
    "y_data_scaled=scaler_y.transform(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Multidimensional regression with Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create a Gaussian Process Regression model for this training data and fit it to the entire dataset as well as to the test data.\n",
    "\n",
    "* No need to create figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "\n",
    "# Write here.\n",
    "\n",
    "#until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you created the GPR model, let's create Surface plots to visualize the approximation when compared with the ground truth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1 is a Surface plot of the ground truth and\n",
    "# the GPR approximation (2 subplots):\n",
    "fig1 = plt.figure(figsize=plt.figaspect(2.0))\n",
    "#\n",
    "# Subplot 1 (top) of Figure 1: ground truth\n",
    "ax1 = fig1.add_subplot(2, 1, 1, projection='3d')\n",
    "#\n",
    "# Don't forget that for a Surface plot we need the data\n",
    "# coming out of meshgrid not in the format of X_data (!)\n",
    "#\n",
    "surf = ax1.plot_surface(xx1, xx2, yy1,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "#\n",
    "# Create axis labels and title:\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(x_1,x_2)$')\n",
    "ax1.set_title(\"Ground truth of %s function\" % function_name)\n",
    "\n",
    "# Subplot 2 (bottom) of Figure 1\n",
    "ax2 = fig1.add_subplot(2, 1, 2, projection='3d')\n",
    "#\n",
    "yy1_data_pred = np.reshape(y_data_pred,np.shape(xx1))\n",
    "\n",
    "surf = ax2.plot_surface(xx1, xx2, yy1_data_pred,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "# Create axis labels and title:\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_zlabel('$f(x_1,x_2)$')\n",
    "ax2.set_title(\"GPR mean for %s function\" % function_name)\n",
    "ax2.scatter(X_train[:,0], X_train[:,1], y_train,\n",
    "            marker='o', color='red',\n",
    "           label=\"training points\")\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot this information via contour plots, and show not only the mean predicted by GPR but also the standard deviation.\n",
    "\n",
    "Note: you can also overlay the 95% confidence intervals in a surface plot, but usually it becomes really cluttered (so we didn't do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# Figure 2\n",
    "fig2 = plt.figure(figsize=plt.figaspect(2.0))\n",
    "# Create Contour plot:\n",
    "ax3 = fig2.add_subplot(3, 1, 1)\n",
    "cset = ax3.contourf(xx1, xx2, yy1, cmap=set_cm)\n",
    "ax3.set_xlabel('$x_1$')\n",
    "ax3.set_ylabel('$x_2$')\n",
    "ax3.set_title(\"Ground truth of %s function\" % function_name)\n",
    "fig2.colorbar(cset, ax=ax3)\n",
    "\n",
    "# Create Contour plot:\n",
    "ax4 = fig2.add_subplot(3, 1, 2)\n",
    "cset = ax4.contourf(xx1, xx2, yy1_data_pred, cmap=set_cm)\n",
    "ax4.set_xlabel('$x_1$')\n",
    "ax4.set_ylabel('$x_2$')\n",
    "ax4.set_title(\"GPR mean for %s function\" % function_name)\n",
    "fig2.colorbar(cset, ax=ax4)\n",
    "\n",
    "# Create Contour plot:\n",
    "ax5 = fig2.add_subplot(3, 1, 3)\n",
    "\n",
    "sigmasigma_data_pred = np.reshape(sigma_data_pred,np.shape(xx1))\n",
    "\n",
    "cset = ax5.contourf(xx1, xx2, sigmasigma_data_pred, cmap=set_cm)\n",
    "ax5.set_xlabel('$x_1$')\n",
    "ax5.set_ylabel('$x_2$')\n",
    "ax5.set_title(\"GPR STDV for %s function\" % function_name)\n",
    "fig2.colorbar(cset, ax=ax5)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it quite remarkable that GPR predicts this function almost perfectly while using only 50 points! This is equivalent to using just $\\sqrt{50}\\approx 7$ points per dimension!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Compute the $R^2$ error metric for the GPR approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "\n",
    "# Write here.\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, **Gaussian Processes are not perfect!**\n",
    "\n",
    "* Try to redo the GPR approximation but now using 75% of the data for training (click [here](##split_dataset) to go to cell of Exercise 1)\n",
    "\n",
    "* You will find that it will take a while to fit the GPR model (training dataset has 1875 points).\n",
    "\n",
    "As we mentioned previously, Gaussian Processes are quite powerful but they are not very scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Multidimensional regression with Artificial Neural Networks\n",
    "\n",
    "As we discovered previously, ANNs behave in a somewhat opposite way: training for data scarce problems can be challenging, but ANNs are very scalable for \"big data\" problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#\n",
    "# Function to create model, required for KerasClassifier when SPECIFYING INPUTS\n",
    "def create_model(input_dimensions=1,neurons1=10,neurons2=10,neurons3=10,neurons4=10,activation='relu',optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=input_dimensions, activation=activation)) # first hidden layer\n",
    "    model.add(Dense(neurons2, activation=activation)) # second hidden layer\n",
    "    #model.add(Dense(neurons3, activation=activation)) # thrid hidden layer\n",
    "    #model.add(Dense(neurons4, activation=activation)) # fourth hidden layer, etc.\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "#\n",
    "# Do you want to look for the best parameters for the Neural Network?\n",
    "# (slower)\n",
    "gridsearch = 0\n",
    "\n",
    "if gridsearch==1:\n",
    "    # create model\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=30, mode='min')\n",
    "#    NN_model = KerasRegressor(build_fn=create_model(input_dimensions=1,neurons=20),\n",
    "#                              callbacks=[early_stopping], validation_data=(scaler.transform(X_test), y_test))\n",
    "    # define the grid search parameters\n",
    "    neurons1 = [5,20,200] # number of neurons in hidden layer 1\n",
    "    neurons2 = [5,10] # number of neurons in hidden layer 2 (if present; uncomment in create_model function)\n",
    "    neurons3 = [10] # number of neurons in hidden layer 3 (if present; uncomment in create_model function)\n",
    "    neurons4 = [10] # number of neurons in hidden layer 4 (if present; uncomment in create_model function)\n",
    "    #\n",
    "    batch_size = [len(X_train)]\n",
    "    #\n",
    "    epochs = [1000]\n",
    "    #\n",
    "    optimizer = ['adam']\n",
    "#    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#    init_mode = ['uniform', 'lecun_uniform', 'normal', 'orthogonal', 'zero', 'one', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']    \n",
    "    #\n",
    "    param_grid = dict(batch_size=batch_size,\n",
    "                      epochs=epochs,neurons1=neurons1,\n",
    "                      neurons2=neurons2,\n",
    "                      #neurons3=neurons3,neurons4=neurons4, # commented out because I am not using them\n",
    "                      optimizer=optimizer)\n",
    "    NN_model = KerasRegressor(build_fn=create_model,\n",
    "                              input_dimensions=np.shape(X_train)[1])\n",
    "    grid = GridSearchCV(estimator=NN_model,\n",
    "                        param_grid=param_grid,\n",
    "                        n_jobs=1, cv=3, iid=False)\n",
    "    grid_result = grid.fit(X_train_scaled, y_train,\n",
    "                           callbacks=[early_stopping],\n",
    "                           validation_data=(X_test_scaled, y_test_scaled))\n",
    "    history = grid_result.best_estimator_.fit(X_train_scaled,\n",
    "                                              y_train,\n",
    "                                              callbacks=[early_stopping],\n",
    "                                              validation_data=(X_test_scaled,\n",
    "                                                               y_test_scaled))\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_,\n",
    "                                 grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "else: # just use a particular Neural Network of choice\n",
    "    # Define early stopping:\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   min_delta=0.0,\n",
    "                                   patience=30,\n",
    "                                   mode='min')\n",
    "    neurons1=200\n",
    "    neurons2=10\n",
    "    NN_model = KerasRegressor(build_fn=create_model,\n",
    "                              input_dimensions=np.shape(X_train)[1],\n",
    "                              neurons1=neurons1,\n",
    "                              neurons2=neurons2,\n",
    "                              batch_size=len(X_train),\n",
    "                              epochs=1000,\n",
    "                              optimizer='adam',\n",
    "                              callbacks=[early_stopping],\n",
    "                              validation_data=(X_test_scaled,\n",
    "                                               y_test_scaled))\n",
    "    #\n",
    "    history = NN_model.fit(X_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the same plots as we made for GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1 is a Surface plot of the ground truth and\n",
    "# the ANN approximation (2 subplots):\n",
    "fig1 = plt.figure(figsize=plt.figaspect(2.0))\n",
    "#\n",
    "# Subplot 1 (top) of Figure 1: ground truth\n",
    "ax1 = fig1.add_subplot(2, 1, 1, projection='3d')\n",
    "#\n",
    "# Don't forget that for a Surface plot we need the data\n",
    "# coming out of meshgrid not in the format of X_data (!)\n",
    "#\n",
    "surf = ax1.plot_surface(xx1, xx2, yy1,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "#\n",
    "# Create axis labels and title:\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(x_1,x_2)$')\n",
    "ax1.set_title(\"Ground truth of %s function\" % function_name)\n",
    "\n",
    "# Subplot 2 (bottom) of Figure 1\n",
    "ax2 = fig1.add_subplot(2, 1, 2, projection='3d')\n",
    "#\n",
    "y_data_pred = history.model.predict(X_data_scaled)\n",
    "yy1_data_pred = np.reshape(scaler_y.inverse_transform(y_data_pred),\n",
    "                           np.shape(xx1)) # note the transformation\n",
    "                                          # of the outputs back to\n",
    "                                          # the original scale\n",
    "\n",
    "surf = ax2.plot_surface(xx1, xx2, yy1_data_pred,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "# Create axis labels and title:\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_zlabel('$f(x_1,x_2)$')\n",
    "ax2.set_title(\"ANN approximation of %s function\" % function_name)\n",
    "ax2.scatter(X_train[:,0], X_train[:,1], y_train,\n",
    "            marker='o', color='red',\n",
    "           label=\"training points\")\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation is not bad when we know a bit about what we are doing ;)\n",
    "\n",
    "* However, it can be challenging to make ANNs as good as GPR for a small number of training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R2 for the ANN model\n",
    "\n",
    "y_test_pred = history.model.predict(X_test_scaled)\n",
    "\n",
    "ANN_r2_value = r2_score(y_test, scaler_y.inverse_transform(y_test_pred))\n",
    "# note the transformation of the outputs back to the original scale\n",
    "\n",
    "print('R2 score for ANN = ', ANN_r2_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training GPR and ANN using different split ratio for training and testing data!\n",
    "\n",
    "* Click [here](##split_dataset) to go to the cell of Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. GPR and ANN with noisy datasets\n",
    "\n",
    "Due to time constraints, I won't do with you the same cases when considering a noisy dataset...\n",
    "\n",
    "But I am including the codes here and we can take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the noisy dataset:\n",
    "random_std = 0.05+0.1*np.random.random(y_data.shape)\n",
    "max_diff = np.max(y_data)-np.min(y_data) # magnitude of signal\n",
    "noise_data = np.random.normal(0,random_std)*max_diff\n",
    "y_noisy_data = y_data + noise_data # Perturb every y_data point with Gaussian noise\n",
    "\n",
    "# Pair up points with their associated noise level (because of train_test_split):\n",
    "Y_noisy_data = np.column_stack((y_noisy_data,noise_data))\n",
    "\n",
    "# Split into 10% training points and the rest for testing:\n",
    "X_train, X_test, Y_noisy_train, Y_noisy_test = train_test_split(X_data,\n",
    "                                    Y_noisy_data, test_size=testset_ratio,\n",
    "                                    random_state=seed) # \"noisy_train\" is a great name for a variable, hein?\n",
    "\n",
    "y_noisy_train = Y_noisy_train[:,0]\n",
    "noise_train = Y_noisy_train[:,1]\n",
    "y_noisy_test = Y_noisy_test[:,0]\n",
    "noise_test = Y_noisy_test[:,1]\n",
    "\n",
    "# Scaling inputs with a Standard Scaler:\n",
    "scaler_x = StandardScaler().fit(X_train)\n",
    "#\n",
    "X_train_scaled=scaler_x.transform(X_train)\n",
    "X_test_scaled=scaler_x.transform(X_test)\n",
    "X_data_scaled=scaler_x.transform(X_data)\n",
    "\n",
    "# Scaling outputs with a Standard Scaler:\n",
    "y_noisy_train = y_noisy_train.reshape(-1, 1)\n",
    "y_noisy_test = y_noisy_test.reshape(-1, 1)\n",
    "y_noisy_data = y_noisy_data.reshape(-1, 1)\n",
    "#\n",
    "scaler_y = MinMaxScaler().fit(y_noisy_train)\n",
    "#\n",
    "y_noisy_train_scaled=scaler_y.transform(y_noisy_train)\n",
    "y_noisy_test_scaled=scaler_y.transform(y_noisy_test)\n",
    "y_noisy_data_scaled=scaler_y.transform(y_noisy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRP\n",
    "# Define the kernel function\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) # This is the standard RBF kernel\n",
    "#kernel = 1.0 * RBF(10, (1e-2, 1e2)) # Same kernel as above\n",
    "                                    #(scikit-learn assumes constant\n",
    "                                    # variance if you just write RBF\n",
    "                                    # without the constant kernel or\n",
    "                                    # without multiplying by 1.0)\n",
    "\n",
    "# Other examples of kernels:\n",
    "#kernel = ExpSineSquared(length_scale=3.0, periodicity=3.14,\n",
    "#                       length_scale_bounds=(0.1, 10.0),\n",
    "#                       periodicity_bounds=(0.1, 10)) * RBF(3.0, (1e-2, 1e2))\n",
    "#kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=1.5)\n",
    "                \n",
    "gp_model = GaussianProcessRegressor(kernel=kernel,\n",
    "                                    alpha=noise_train.ravel()**2,\n",
    "                                    n_restarts_optimizer=20) # using a small alpha\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp_model.fit(X_train, y_noisy_train)\n",
    "\n",
    "# Make the prediction on the entire dataset (for plotting)\n",
    "y_noisy_data_pred, sigma_noisy_data_pred = gp_model.predict(X_data,\n",
    "                                                            return_std=True) # also output the uncertainty (standard deviation)\n",
    "\n",
    "# Predict for test set (for error metric)\n",
    "y_noisy_test_pred, sigma_noisy_test_pred = gp_model.predict(X_test,\n",
    "                                                            return_std=True) # also output the uncertainty (standard deviation)\n",
    "\n",
    "# PLOTS\n",
    "# Figure 1\n",
    "# Subplot 1\n",
    "fig1 = plt.figure(figsize=plt.figaspect(2.0))\n",
    "ax1 = fig1.add_subplot(2, 1, 1, projection='3d')\n",
    "surf = ax1.plot_surface(xx1, xx2, yy1,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(x_1,x_2)$')\n",
    "ax1.set_title(\"Ground truth of %s function\" % function_name)\n",
    "ax1.scatter(X_train[:,0], X_train[:,1], y_noisy_train,\n",
    "            marker='o', color='red',\n",
    "           label=\"training points\")\n",
    "ax1.legend(loc='lower left')\n",
    "\n",
    "# Subplot 2\n",
    "ax2 = fig1.add_subplot(2, 1, 2, projection='3d')\n",
    "yy1_noisy_data_pred = np.reshape(y_noisy_data_pred,np.shape(xx1))\n",
    "surf = ax2.plot_surface(xx1, xx2,\n",
    "                        yy1_noisy_data_pred,\n",
    "                        cmap=set_cm, alpha=0.8,\n",
    "                        linewidth=0, antialiased=False)\n",
    "# Create axis labels and title:\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_zlabel('$f(x_1,x_2)$')\n",
    "ax2.set_title(\"GPR mean for %s function\" % function_name)\n",
    "ax2.scatter(X_train[:,0], X_train[:,1], y_noisy_train,\n",
    "            marker='o', color='red',\n",
    "           label=\"training points\")\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# Figure 2\n",
    "fig2 = plt.figure(figsize=plt.figaspect(2.0))\n",
    "# Create Contour plot:\n",
    "ax3 = fig2.add_subplot(3, 1, 1)\n",
    "cset = ax3.contourf(xx1, xx2, yy1, cmap=set_cm)\n",
    "ax3.set_xlabel('$x_1$')\n",
    "ax3.set_ylabel('$x_2$')\n",
    "ax3.set_title(\"Ground truth of %s function\" % function_name)\n",
    "fig2.colorbar(cset, ax=ax3)\n",
    "\n",
    "# Create Contour plot:\n",
    "ax4 = fig2.add_subplot(3, 1, 2)\n",
    "cset = ax4.contourf(xx1, xx2,\n",
    "                    yy1_noisy_data_pred,\n",
    "                    cmap=set_cm)\n",
    "ax4.set_xlabel('$x_1$')\n",
    "ax4.set_ylabel('$x_2$')\n",
    "ax4.set_title(\"GPR mean for %s function\" % function_name)\n",
    "fig2.colorbar(cset, ax=ax4)\n",
    "\n",
    "# Create Contour plot:\n",
    "ax5 = fig2.add_subplot(3, 1, 3)\n",
    "sigmasigma_noisy_data_pred = np.reshape(sigma_noisy_data_pred,\n",
    "                                        np.shape(xx1))\n",
    "cset = ax5.contourf(xx1, xx2,\n",
    "                    sigmasigma_noisy_data_pred,\n",
    "                    cmap=set_cm)\n",
    "ax5.set_xlabel('$x_1$')\n",
    "ax5.set_ylabel('$x_2$')\n",
    "ax5.set_title(\"GPR STDV for %s function\" % function_name)\n",
    "fig2.colorbar(cset, ax=ax5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Error metric\n",
    "# Compute R2 for the GP model\n",
    "gp_r2_value = r2_score(y_noisy_test, y_noisy_test_pred)\n",
    "\n",
    "print('R2 score for GPR = ', gp_r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now for the ANN:\n",
    "\n",
    "# Function to create model, required for KerasClassifier when SPECIFYING INPUTS\n",
    "def create_model(input_dimensions=1,neurons1=10,neurons2=10,neurons3=10,neurons4=10,activation='relu',optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=input_dimensions, activation=activation)) # first hidden layer\n",
    "    model.add(Dense(neurons2, activation=activation)) # second hidden layer\n",
    "    #model.add(Dense(neurons3, activation=activation)) # thrid hidden layer\n",
    "    #model.add(Dense(neurons4, activation=activation)) # fourth hidden layer, etc.\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "#\n",
    "# Do you want to look for the best parameters for the Neural Network?\n",
    "# (slower)\n",
    "gridsearch = 0\n",
    "\n",
    "if gridsearch==1:\n",
    "    # create model\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=30, mode='min')\n",
    "#    NN_model = KerasRegressor(build_fn=create_model(input_dimensions=1,neurons=20),\n",
    "#                              callbacks=[early_stopping], validation_data=(scaler.transform(X_test), y_test))\n",
    "    # define the grid search parameters\n",
    "    neurons1 = [5,20,200] # number of neurons in hidden layer 1\n",
    "    neurons2 = [5,10] # number of neurons in hidden layer 2 (if present; uncomment in create_model function)\n",
    "    neurons3 = [10] # number of neurons in hidden layer 3 (if present; uncomment in create_model function)\n",
    "    neurons4 = [10] # number of neurons in hidden layer 4 (if present; uncomment in create_model function)\n",
    "    #\n",
    "    batch_size = [len(X_train)]\n",
    "    #\n",
    "    epochs = [1000]\n",
    "    #\n",
    "    optimizer = ['adam']\n",
    "#    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#    init_mode = ['uniform', 'lecun_uniform', 'normal', 'orthogonal', 'zero', 'one', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']    \n",
    "    #\n",
    "    param_grid = dict(batch_size=batch_size,\n",
    "                      epochs=epochs,neurons1=neurons1,\n",
    "                      neurons2=neurons2,\n",
    "                      #neurons3=neurons3,neurons4=neurons4, # commented out because I am not using them\n",
    "                      optimizer=optimizer)\n",
    "    NN_model = KerasRegressor(build_fn=create_model,\n",
    "                              input_dimensions=np.shape(X_train)[1])\n",
    "    grid = GridSearchCV(estimator=NN_model,\n",
    "                        param_grid=param_grid,\n",
    "                        n_jobs=1, cv=3, iid=False)\n",
    "    grid_result = grid.fit(X_train_scaled, y_train,\n",
    "                           callbacks=[early_stopping],\n",
    "                           validation_data=(X_test_scaled, y_noisy_test_scaled))\n",
    "    history = grid_result.best_estimator_.fit(X_train_scaled,\n",
    "                                              y_train,\n",
    "                                              callbacks=[early_stopping],\n",
    "                                              validation_data=(X_test_scaled,\n",
    "                                                               y_noisy_test_scaled))\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_,\n",
    "                                 grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "else: # just use a particular Neural Network of choice\n",
    "    # Define early stopping:\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   min_delta=0.0,\n",
    "                                   patience=30,\n",
    "                                   mode='min')\n",
    "    neurons1=200\n",
    "    neurons2=10\n",
    "    NN_model = KerasRegressor(build_fn=create_model,\n",
    "                              input_dimensions=np.shape(X_train)[1],\n",
    "                              neurons1=neurons1,\n",
    "                              neurons2=neurons2,\n",
    "                              batch_size=len(X_train),\n",
    "                              epochs=1000,\n",
    "                              optimizer='adam',\n",
    "                              callbacks=[early_stopping],\n",
    "                              validation_data=(X_test_scaled,\n",
    "                                               y_noisy_test_scaled))\n",
    "    #\n",
    "    history = NN_model.fit(X_train_scaled, y_noisy_train_scaled)\n",
    "#    \n",
    "#   \n",
    "# PLOTS\n",
    "# Figure 1 is a Surface plot of the ground truth and\n",
    "# the ANN approximation (2 subplots):\n",
    "fig1 = plt.figure(figsize=plt.figaspect(2.0))\n",
    "#\n",
    "# Subplot 1 (top) of Figure 1: ground truth\n",
    "ax1 = fig1.add_subplot(2, 1, 1, projection='3d')\n",
    "#\n",
    "# Don't forget that for a Surface plot we need the data\n",
    "# coming out of meshgrid not in the format of X_data (!)\n",
    "#\n",
    "surf = ax1.plot_surface(xx1, xx2, yy1,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "#\n",
    "# Create axis labels and title:\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$f(x_1,x_2)$')\n",
    "ax1.set_title(\"Ground truth of %s function\" % function_name)\n",
    "ax1.scatter(X_train[:,0], X_train[:,1], y_noisy_train,\n",
    "            marker='o', color='red',\n",
    "            label=\"training points\")\n",
    "ax1.legend(loc='lower left')\n",
    "\n",
    "# Subplot 2 (bottom) of Figure 1\n",
    "ax2 = fig1.add_subplot(2, 1, 2, projection='3d')\n",
    "#\n",
    "y_noisy_data_pred = history.model.predict(X_data_scaled)\n",
    "yy1_noisy_data_pred = np.reshape(scaler_y.inverse_transform(y_noisy_data_pred),\n",
    "                           np.shape(xx1)) # note the transformation\n",
    "                                          # of the outputs back to\n",
    "                                          # the original scale\n",
    "\n",
    "surf = ax2.plot_surface(xx1, xx2, yy1_noisy_data_pred,\n",
    "                       cmap=set_cm, alpha=0.8,\n",
    "                       linewidth=0, antialiased=False)\n",
    "# Create axis labels and title:\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_zlabel('$f(x_1,x_2)$')\n",
    "ax2.set_title(\"ANN approximation of %s function\" % function_name)\n",
    "ax2.scatter(X_train[:,0], X_train[:,1], y_noisy_train,\n",
    "            marker='o', color='red',\n",
    "           label=\"training points\")\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute R2 for the ANN model\n",
    "y_noisy_test_pred = history.model.predict(X_test_scaled)\n",
    "ANN_r2_value = r2_score(y_noisy_test, scaler_y.inverse_transform(y_noisy_test_pred))\n",
    "# note the transformation of the outputs back to the original scale\n",
    "\n",
    "print('R2 score for ANN = ', ANN_r2_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget to \"play\" with this notebook\n",
    "\n",
    "* **Try learning different functions!**: go to the beginning of the notebook and change the benchmark function to be learned (e.g. Levy function)\n",
    "\n",
    "\n",
    "* Use different noise levels\n",
    "\n",
    "\n",
    "* Try different kernels for the GPR\n",
    "\n",
    "\n",
    "* Do a gridsearch to find better parameters for the ANN\n",
    "\n",
    "\n",
    "* Investigate the role of \"early stopping\" in the ANN\n",
    "\n",
    "\n",
    "There are so many things that we can do to go deeper in the topic! Have some fun with it!\n",
    "\n",
    "**Important Note:** Despite this notebook being dedicated to 3D surfaces (2 features and 1 target), the algorithms are general. They can perform regression for any dimensionality of features and targets. The codes do not change significantly, but visualizing data can be a challenge. The simplest solution is to project the space back to a 3D space. You will do that in the Final Project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multidimensional classification\n",
    "\n",
    "Here's some good news:\n",
    "\n",
    "* If you understood most of what we discussed for regression with supervised learning... Then, performing classification with supervised learning is very similar from a user standpoint.\n",
    "\n",
    "But first: what is classification?\n",
    "\n",
    "* Classification problems occur when the target (output) is **discrete**, instead of being a continuous variable like we did in regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A simple classification problem that is very common to consider when first learning about this topic is the Iris dataset created by UCI researchers in 1936: http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "* I downloaded the .CSV file from Kaggle and it is included in the docs folder of this GitHub repository.\n",
    "\n",
    "Therefore, we can load that .CVS file into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset.\n",
    "\n",
    "iris_data = pd.read_csv('docs/Iris.csv')\n",
    "\n",
    "print(iris_data)\n",
    "\n",
    "# A short note: in fact, the Iris dataset is SO common, that\n",
    "# Scikit-learn even has it saved and it can be loaded directly\n",
    "# from the scikit-learn code:\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris() # load Iris dataset that already exists in Scikit-learn\n",
    "# Convert it to a Pandas Dataframe\n",
    "data1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "# You can print(data1) and see that it is the same dataset.\n",
    "\n",
    "# But I think it is informative to understand the usual process\n",
    "# in machine learning: someone gives us a dataset (or we download\n",
    "# it from somewhere) and we usually load it with Pandas due to its\n",
    "# versatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start a new problem, the first step is:\n",
    "\n",
    "* Get to know the dataset.\n",
    "\n",
    "What are the features? And the targets? What kind of data preprocessing should we do? What do we know *before* using machine learning tools on the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here's a nice figure to understand the dataset (source: https://www.ritchieng.com/machine-learning-iris-dataset/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=docs/iris_dataset_explanation.png width=300px></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite simple! It has:\n",
    "\n",
    "* 4 Features (inputs): sepal length, sepal width, petal length, petal width\n",
    "\n",
    "\n",
    "* 1 Target (output): iris species (3 different species with 50 samples each)\n",
    "\n",
    "In other words, the dataset has 150 samples of 3 different species of iris (50 samples per species) and each flower has 4 features that characterize it: the length and width of the petal and the sepal (see figure above).\n",
    "\n",
    "**Our task**: Learn from part of the dataset how to classify a new iris flower into one of the 3 species!\n",
    "\n",
    "Note: This example is straight out of Scikit-learn (but simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Let's only use the first two features (ignoring the other 2).\n",
    "X = iris_data.loc[:, ['SepalLengthCm','SepalWidthCm'] ].values\n",
    "y_string = iris_data.loc[:, 'Species'].values\n",
    "\n",
    "print('y_string = ',y_string,'\\n\\n')\n",
    "\n",
    "# Importantly, we should convert our target (output) data from\n",
    "# string to integers! In this case we have three discrete target\n",
    "# values, so we should convert to 0, 1 and 2\n",
    "\n",
    "# Pandas makes this very easy for us. Just one line!\n",
    "\n",
    "y = pd.Categorical(pd.factorize(y_string)[0])\n",
    "\n",
    "print('y = ',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data (this dataset is quite simple)\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svm_model = svm.SVC(kernel='rbf', gamma=0.7, C=C)\n",
    "\n",
    "svm_model.fit(X, y)\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, ax = plt.subplots()\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X1, X2 = X[:, 0], X[:, 1]\n",
    "\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1 # define min and max of feature 0\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1 # define min and max of feature 0\n",
    "xx1, xx2 = np.meshgrid(np.arange(X1_min, X1_max, 0.02),\n",
    "                       np.arange(X2_min, X2_max, 0.02))\n",
    "\n",
    "Y = svm_model.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Y = Y.reshape(xx1.shape)\n",
    "\n",
    "ax.contourf(xx1, xx2, Y, cmap=cm.coolwarm, alpha=0.8)\n",
    "\n",
    "#plot_contours(ax, svm_model, xx1, xx2,\n",
    "#              cmap=cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X1, X2, c=y, cmap=cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx1.min(), xx1.max())\n",
    "ax.set_ylim(xx2.min(), xx2.max())\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title('SVC with RBF kernel')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's now plot for all 4 features, instead of considering only the first 2.\n",
    "\n",
    "* Let's also use a different classifier. Instead of Support Vector Machines (SVM), we will use Decision Trees.\n",
    "\n",
    "The following is also an example from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load all the features and the target variables:\n",
    "X = iris_data.iloc[:, 1:5 ].values\n",
    "feature_names = iris_data.columns.values[1:5] # not including the \"ID\" column\n",
    "\n",
    "y_strings = iris_data.loc[:, 'Species'].values\n",
    "target_names = iris_data.loc[:, 'Species'].unique()\n",
    "\n",
    "print(feature_names)\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "fig, ax = plt.subplots() # open figure\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    #X_temp = iris.data[:, pair]\n",
    "    #y_temp = iris.target\n",
    "\n",
    "    X_temp = X[:, pair]\n",
    "    \n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X_temp, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x1_min, x1_max = X_temp[:, 0].min() - 1, X_temp[:, 0].max() + 1\n",
    "    x2_min, x2_max = X_temp[:, 1].min() - 1, X_temp[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, plot_step),\n",
    "                         np.arange(x2_min, x2_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Y = clf.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    Y = Y.reshape(xx1.shape)\n",
    "    cs = plt.contourf(xx1, xx2, Y, cmap=cm.RdYlBu)\n",
    "\n",
    "    plt.xlabel(feature_names[pair[0]])\n",
    "    plt.ylabel(feature_names[pair[1]])\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X_temp[idx, 0], X_temp[idx, 1], c=color,\n",
    "                    label=target_names[i],cmap=plt.cm.RdYlBu,\n",
    "                    edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv3",
   "language": "python",
   "name": "mlenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
